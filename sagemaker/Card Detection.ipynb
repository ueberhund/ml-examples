{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker Card Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook attempts to do my own job of image classification. This notebook pulls a list of  photos of playing cards and creates a classification model that can be used to predict the card that is in the photo.\n",
    "\n",
    "This notebook is based off the following sources:\n",
    "\n",
    "[1] https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_mscoco_multi_label/Image-classification-multilabel-lst.ipynb\n",
    "\n",
    "[2] https://github.com/aws-samples/aws-deeplens-reinvent-2019-workshops/blob/master/AIM405-Advanced/Lab2/lab2-image-classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with setting up some standard stuff to run Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'card'\n",
    "\n",
    "print('using bucket %s'%bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the image classification training image used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'image-classification', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by cloning a github repo containing playing cards. These are the cards we'll be using for our trainng.\n",
    "\n",
    "Extract the data and save to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install pycocotools\n",
    "!pip install scikit-image\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/lordloh/playing-cards.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a data file that contains the image and the category it belongs to. These files will be used as the input data to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy import ndarray\n",
    "import skimage as sk\n",
    "from skimage import transform\n",
    "from skimage import util\n",
    "\n",
    "def random_rotation(image_array: ndarray):\n",
    "    # pick a random degree of rotation between 90% on the left and 90% on the right\n",
    "    random_degree = random.uniform(-90, 90)\n",
    "    return sk.transform.rotate(image_array, random_degree)\n",
    "\n",
    "def random_noise(image_array: ndarray):\n",
    "    # add random noise to the image\n",
    "    return sk.util.random_noise(image_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have enough images, so generate some new images using data augmentation per this post: \n",
    "\n",
    "https://medium.com/@thimblot/data-augmentation-boost-your-image-dataset-with-few-lines-of-python-155c2dc1baec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pycocotools.coco import COCO\n",
    "import random\n",
    "import skimage.io\n",
    "\n",
    "DATA_DIR = './playing-cards/img'\n",
    "num_files_desired = 1000\n",
    "\n",
    "SEARCH_CRITERION = '**/*.jpg'\n",
    "base_images = glob.glob(os.path.join(DATA_DIR, SEARCH_CRITERION), recursive=True)\n",
    "\n",
    "available_transformations = {\n",
    "    'rotate': random_rotation,\n",
    "    'noise': random_noise\n",
    "}\n",
    "\n",
    "num_generated_files = 0\n",
    "while num_generated_files <= num_files_desired:\n",
    "    image_path = random.choice(base_images)\n",
    "    image_to_transform = sk.io.imread(image_path)\n",
    "    \n",
    "    num_transformations_to_apply = random.randint(1, len(available_transformations))\n",
    "\n",
    "    num_transformations = 0\n",
    "    \n",
    "    while num_transformations <= num_transformations_to_apply:\n",
    "        # choose a random transformation to apply for a single image\n",
    "        key = random.choice(list(available_transformations))\n",
    "        transformed_image = available_transformations[key](image_to_transform)\n",
    "        num_transformations += 1\n",
    "        \n",
    "    # define a name for our new file\n",
    "    new_file_path = '%s/%s-aug-%s.jpg' % (DATA_DIR, image_path[20:34], random.randrange(1,50000))\n",
    "\n",
    "    # write image to the disk\n",
    "    sk.io.imsave(new_file_path, transformed_image)\n",
    "    num_generated_files += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#rm -r ./playing-cards\n",
    "mkdir ./playing-cards/img/train\n",
    "mkdir ./playing-cards/img/validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pycocotools.coco import COCO\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_data_file(image_list, image_type):\n",
    "    with open('image-' + image_type + '.lst', 'w') as fp:\n",
    "        for ind in enumerate(image_list):\n",
    "            image_path = ind[1]\n",
    "            \n",
    "            #Move file to a new path for ease in moving to S3\n",
    "            filename = image_path[20:-4]\n",
    "            newpath = './playing-cards/img/' + image_type + '/' + filename + '.jpg'\n",
    "        \n",
    "            os.rename(image_path, newpath)\n",
    "            \n",
    "            fp.write(str(ind[0]) + '\\t')\n",
    "            if image_path.find('[W') > -1:\n",
    "                fp.write('13\\t')\n",
    "            elif image_path.find('0]') > -1:\n",
    "                fp.write('0\\t')\n",
    "            elif image_path.find('2]') > -1:\n",
    "                fp.write('1\\t')\n",
    "            elif image_path.find('3]') > -1:\n",
    "                fp.write('2\\t')\n",
    "            elif image_path.find('4]') > -1:\n",
    "                fp.write('3\\t')\n",
    "            elif image_path.find('5]') > -1:\n",
    "                fp.write('4\\t')\n",
    "            elif image_path.find('6]') > -1:\n",
    "                fp.write('5\\t')\n",
    "            elif image_path.find('7]') > -1:\n",
    "                fp.write('6\\t')\n",
    "            elif image_path.find('8]') > -1:\n",
    "                fp.write('7\\t')\n",
    "            elif image_path.find('9]') > -1:\n",
    "                fp.write('8\\t')\n",
    "            elif image_path.find('J]') > -1:\n",
    "                fp.write('9\\t')\n",
    "            elif image_path.find('Q]') > -1:\n",
    "                fp.write('10\\t')\n",
    "            elif image_path.find('K]') > -1:\n",
    "                fp.write('11\\t')\n",
    "            elif image_path.find('A]') > -1:\n",
    "                fp.write('12\\t')\n",
    "            \n",
    "            fp.write(filename + '.jpg')\n",
    "            fp.write('\\n')\n",
    "        fp.close()\n",
    "        \n",
    "        \n",
    "DATA_DIR = './playing-cards/img'\n",
    "\n",
    "SEARCH_CRITERION = '**/*.jpg'\n",
    "base_images = glob.glob(os.path.join(DATA_DIR, SEARCH_CRITERION), recursive=True)\n",
    "\n",
    "random.shuffle(base_images)\n",
    "\n",
    "train_images, validate_images = train_test_split(base_images, test_size=0.33, random_state=0)\n",
    "\n",
    "create_data_file(train_images, 'train')\n",
    "create_data_file(validate_images, 'validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-create the train_images and validate_images files, since we moved the files\n",
    "SEARCH_CRITERION = '**/*.jpg'\n",
    "train_images = glob.glob(os.path.join(DATA_DIR + '/train/', SEARCH_CRITERION), recursive=True)\n",
    "validate_images = glob.glob(os.path.join(DATA_DIR + '/validate/', SEARCH_CRITERION), recursive=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some sample images to make sure everything looks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.display import Image\n",
    "\n",
    "rand_image = random.randrange(1,len(train_images))\n",
    "print(train_images[rand_image])\n",
    "Image(train_images[rand_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push files to S3 in preparation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four channels: train, validation, train_lst, and validation_lst\n",
    "s3train = 's3://{}/{}/train/'.format(bucket, prefix)\n",
    "s3train_lst = 's3://{}/{}/train_lst/'.format(bucket, prefix)\n",
    "s3validation = 's3://{}/{}/validation/'.format(bucket, prefix)\n",
    "s3validation_lst = 's3://{}/{}/validation_lst/'.format(bucket, prefix)\n",
    "\n",
    "# upload the image files to train and validation channels\n",
    "!aws s3 cp $DATA_DIR/train $s3train --recursive --quiet\n",
    "!aws s3 cp $DATA_DIR/validate $s3validation --recursive --quiet\n",
    "!\n",
    "# upload the lst files to train_lst and validation_lst channels\n",
    "!aws s3 cp image-train.lst $s3train_lst --quiet\n",
    "!aws s3 cp image-validate.lst $s3validation_lst --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin the training of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "multilabel_ic = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.p3.2xlarge',\n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_ic.set_hyperparameters(num_layers=200,\n",
    "                             use_pretrained_model=1,\n",
    "                             image_shape = \"3,224,224\",\n",
    "                             num_classes=14,\n",
    "                             mini_batch_size=64,\n",
    "                             epochs=20,\n",
    "                             resize=256,\n",
    "                             learning_rate=0.001,\n",
    "                             num_training_samples=len(train_images),\n",
    "                             use_weighted_loss=1,\n",
    "                             augmentation_type = 'crop_color_transform',\n",
    "                             precision_dtype='float16',\n",
    "                             multi_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3train, distribution='FullyReplicated', \n",
    "                        content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "train_data_lst = sagemaker.session.s3_input(s3train_lst, distribution='FullyReplicated', \n",
    "                        content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "\n",
    "validation_data = sagemaker.session.s3_input(s3validation, distribution='FullyReplicated', \n",
    "                        content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "validation_data_lst = sagemaker.session.s3_input(s3validation_lst, distribution='FullyReplicated', \n",
    "                        content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data, 'train_lst': train_data_lst, \n",
    "                        'validation_lst': validation_data_lst}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multilabel_ic.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up an endpoint where we can make inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_classifier = multilabel_ic.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a random image from our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.display import Image\n",
    "import json\n",
    "\n",
    "rand_image = random.randrange(1,len(validate_images)-1)\n",
    "print(validate_images[rand_image])\n",
    "\n",
    "with open(validate_images[rand_image], 'rb') as image:\n",
    "    f = image.read()\n",
    "    b = bytearray(f)\n",
    "ic_classifier.content_type = 'application/x-image'\n",
    "results = ic_classifier.predict(b)\n",
    "\n",
    "\n",
    "prob = json.loads(results)\n",
    "#print(prob)\n",
    "classes = ['10', '2', '3', '4', '5', '6', '7', '8', '9', 'J', 'Q', 'K', 'Joker']\n",
    "\n",
    "predicted_class = ''\n",
    "predicted_prob = ''\n",
    "for idx, val in enumerate(classes):\n",
    "    if predicted_prob == '':\n",
    "        predicted_class = classes[idx]\n",
    "        predicted_prob = prob[idx]\n",
    "    \n",
    "    if predicted_prob < prob[idx]:\n",
    "        predicted_class = classes[idx]\n",
    "        predicted_prob = prob[idx]\n",
    "    \n",
    "    #print('%s:%f '%(classes[idx], prob[idx]), end='')\n",
    "\n",
    "print('%s:%f '%(predicted_class, predicted_prob), end='')\n",
    "Image(validate_images[rand_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import json\n",
    "\n",
    "test_image = './test/IMG_0253.jpeg'\n",
    "\n",
    "with open(test_image, 'rb') as image:\n",
    "    f = image.read()\n",
    "    b = bytearray(f)\n",
    "ic_classifier.content_type = 'application/x-image'\n",
    "results = ic_classifier.predict(b)\n",
    "\n",
    "\n",
    "prob = json.loads(results)\n",
    "#print(prob)\n",
    "classes = ['10', '2', '3', '4', '5', '6', '7', '8', '9', 'J', 'Q', 'K', 'Joker']\n",
    "\n",
    "predicted_class = ''\n",
    "predicted_prob = ''\n",
    "for idx, val in enumerate(classes):\n",
    "    if predicted_prob == '':\n",
    "        predicted_class = classes[idx]\n",
    "        predicted_prob = prob[idx]\n",
    "    \n",
    "    if predicted_prob < prob[idx]:\n",
    "        predicted_class = classes[idx]\n",
    "        predicted_prob = prob[idx]\n",
    "    \n",
    "    #print('%s:%f '%(classes[idx], prob[idx]), end='')\n",
    "\n",
    "print('%s:%f '%(predicted_class, predicted_prob), end='')\n",
    "Image(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model does a pretty good job at predicting the right class. This should give you an idea of how to create an image-based model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_classifier.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
